# -*- coding: utf-8 -*-
"""ejecutar job spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y5OoaLROm2_BYWeonIRGudR7yfukOYME
"""

from pyspark import SparkContext
sc = SparkContext("local", "NumericApp")

# üß≠ Paso 1: Carga un RDD num√©rico desde una lista
numeros = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd_numerico = sc.parallelize(numeros)

# üìú Paso 2: Aplica una o m√°s transformaciones
# Primero, filtramos para quedarnos solo con los n√∫meros pares.
rdd_pares = rdd_numerico.filter(lambda x: x % 2 == 0)

# Luego, transformamos cada n√∫mero par elev√°ndolo al cuadrado.
rdd_cuadrados = rdd_pares.map(lambda x: x * x)

# ‚è≥ Paso 3: Confirma que a√∫n no se ha ejecutado nada
# Si ejecutas este c√≥digo hasta aqu√≠, Spark no ha procesado ning√∫n dato.
# Solo ha construido un "plan" o DAG (Grafo Ac√≠clico Dirigido) de lo que debe hacer.
# 'rdd_cuadrados' es solo un puntero a este plan, no contiene el resultado.
print("Hasta ahora solo se ha definido el plan (DAG). A√∫n no se inicia el Job.")
print('-' * 70)

# üí• Paso 4: Ejecuta una acci√≥n como sum()
# La acci√≥n 'sum()' DESENCADENA la ejecuci√≥n del Job.
# Spark toma el DAG, lo optimiza y lo env√≠a al cl√∫ster para su c√≥mputo.
suma_total = rdd_cuadrados.sum()

# Ahora s√≠ tenemos un resultado final.
print(f"El Job se ha ejecutado. La suma de los cuadrados de los pares es: {suma_total}")
# Resultado esperado: 4 + 16 + 36 + 64 + 100 = 220

sc.stop()
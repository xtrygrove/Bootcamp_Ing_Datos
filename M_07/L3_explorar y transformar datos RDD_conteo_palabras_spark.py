# -*- coding: utf-8 -*-
"""conteo palabras spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17v1B_p5-Hsdt3FVSWvEILBqorMALP5DG
"""

from pyspark import SparkContext

# Check if a SparkContext already exists, and stop it if it does.
if 'sc' in globals() and sc:
    sc.stop()

sc = SparkContext("local", "WordCountApp")

# Datos de entrada (una lista de frases)
texto = [
    "Explora y transforma datos con RDDs",
    "Una de las claves para trabajar con datos distribuidos en Spark es conocer cÃ³mo manipular RDDs",
    "Esta actividad te permitirÃ¡ practicar con datos reales y aplicar operaciones fundamentales"
]

# ðŸ¦¾ Paso 1: Carga la lista usando sc.parallelize()
rdd_inicial = sc.parallelize(texto)

# ìª¼ Paso 2: Utiliza flatMap() para separar el texto en palabras
# Se convierte todo a minÃºsculas para un conteo uniforme.
palabras_rdd = rdd_inicial.flatMap(lambda linea: linea.lower().split(" "))

# ðŸ—‘ï¸ Paso 3: Aplica filter() para eliminar palabras con menos de 4 letras
palabras_filtradas_rdd = palabras_rdd.filter(lambda palabra: len(palabra) >= 4)

# ðŸ—ºï¸ Paso 4: Usa map() para crear pares (palabra, 1)
pares_rdd = palabras_filtradas_rdd.map(lambda palabra: (palabra, 1))

# ðŸ”¢ Paso 5: Aplica reduceByKey() para contar la frecuencia de cada palabra
conteo_rdd = pares_rdd.reduceByKey(lambda acumulador, valor: acumulador + valor)

# ðŸ“Š Paso 6: Ordena los resultados con sortBy()
# Se ordena por el segundo elemento del par (el conteo), de mayor a menor (ascending=False).
conteo_ordenado_rdd = conteo_rdd.sortBy(lambda par: par[1], ascending=False)

# âœ¨ Paso 7: Muestra los 5 mÃ¡s frecuentes con take(5)
resultado = conteo_ordenado_rdd.take(5)

# Imprime el resultado final
print("Las 5 palabras mÃ¡s frecuentes (con 4 o mÃ¡s letras) son:")
for (palabra, conteo) in resultado:
    print(f"-> {palabra}: {conteo}")


sc.stop()
# -*- coding: utf-8 -*-
"""Segmentación de clientes con K-Means.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RofvZm_t1tRrId9B4EJPbesAlfSYe0cY
"""

# !pip install pyspark -q

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml import Pipeline

spark = (SparkSession.builder
         .appName("KMeansClientes")
         .getOrCreate())

# 1) Dataset de ejemplo
data = spark.createDataFrame([
    # edad, ingresos_mensuales, frecuencia_compra_mensual
    (23,  550,  2),
    (25,  700,  3),
    (27,  750,  2),
    (35,  1600, 6),
    (38,  1800, 7),
    (41,  1750, 6),
    (52,  4000, 10),
    (55,  4200, 11),
    (58,  3900, 9),
    (45,  2200, 5),
    (47,  2500, 5),
    (28,  850,  3),
    (31,  1200, 4),
    (60,  5200, 12),
    (22,  500,  2),
], ["edad", "ingresos", "frecuencia"])

# 2) Ensamble de features → VectorAssembler (+ escalado)
features = ["edad", "ingresos", "frecuencia"]
assembler = VectorAssembler(inputCols=features, outputCol="features_raw")
scaler = StandardScaler(inputCol="features_raw", outputCol="features", withStd=True, withMean=False)

# 3) Modelo KMeans con K=4
kmeans = KMeans(featuresCol="features", predictionCol="prediction").setK(4).setSeed(42)

# 4) Pipeline: ensamblar → escalar → clusterizar
pipeline = Pipeline(stages=[assembler, scaler, kmeans])
model = pipeline.fit(data)

# 5) Predicciones (clúster asignado a cada fila)
pred = model.transform(data)
pred.select("edad","ingresos","frecuencia","prediction").show(truncate=False)

# 6) Métrica de cohesión: Silhouette (rango aprox. -1 a 1)
evaluator = ClusteringEvaluator(featuresCol="features", predictionCol="prediction", metricName="silhouette")
sil = evaluator.evaluate(pred)
print(f"Silhouette score: {sil:.4f}")

# 7) Centros de clúster
k_model = model.stages[-1]  # último stage del pipeline es KMeansModel
centers = k_model.clusterCenters()
print("\nCentros de los clústeres (espacio escalado):")
for i, c in enumerate(centers):
    print(f"  Cluster {i}: {c}")

# Búsqueda rápida de K por "codo" con SSE (cost)
def inertia_for_k(k):
    km = KMeans(featuresCol="features", predictionCol="prediction", k=k, seed=42)
    m = Pipeline(stages=[assembler, scaler, km]).fit(data)
    return m.stages[-1].summary.trainingCost

for k in range(2, 8):
    cost = inertia_for_k(k)
    print(f"K={k} -> SSE (trainingCost): {cost:.2f}")

spark.stop()  #cerrar la sesión